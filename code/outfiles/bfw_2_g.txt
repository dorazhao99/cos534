{'batchsize': 32, 'labels_train': '../data/bfw/bfw_fleiss/bfw_gender_2.pkl', 'labels_val': '../data/bfw/bfw_fleiss/bfw_gender_1.pkl', 'labels_test': None, 'num_epochs': 10, 'lr': 0.0003, 'print_freq': 50, 'num_classes': 2, 'model_path': None, 'dtype': torch.float32, 'outdir': '../results/bfw/fleiss/bfw_gender_2'} 

Training on cuda:0
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer2.3.conv1.weight
	 layer2.3.bn1.weight
	 layer2.3.bn1.bias
	 layer2.3.conv2.weight
	 layer2.3.bn2.weight
	 layer2.3.bn2.bias
	 layer2.3.conv3.weight
	 layer2.3.bn3.weight
	 layer2.3.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer3.3.conv1.weight
	 layer3.3.bn1.weight
	 layer3.3.bn1.bias
	 layer3.3.conv2.weight
	 layer3.3.bn2.weight
	 layer3.3.bn2.bias
	 layer3.3.conv3.weight
	 layer3.3.bn3.weight
	 layer3.3.bn3.bias
	 layer3.4.conv1.weight
	 layer3.4.bn1.weight
	 layer3.4.bn1.bias
	 layer3.4.conv2.weight
	 layer3.4.bn2.weight
	 layer3.4.bn2.bias
	 layer3.4.conv3.weight
	 layer3.4.bn3.weight
	 layer3.4.bn3.bias
	 layer3.5.conv1.weight
	 layer3.5.bn1.weight
	 layer3.5.bn1.bias
	 layer3.5.conv2.weight
	 layer3.5.bn2.weight
	 layer3.5.bn2.bias
	 layer3.5.conv3.weight
	 layer3.5.bn3.weight
	 layer3.5.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fc.weight
	 fc.bias

Epoch 1/10
----------
Iter 0/201 (Epoch 0), Train Loss = 0.708
Iter 50/201 (Epoch 0), Train Loss = 0.301
Iter 100/201 (Epoch 0), Train Loss = 0.220
Iter 150/201 (Epoch 0), Train Loss = 0.405
Iter 200/201 (Epoch 0), Train Loss = 0.670
train Loss: 0.2332 Acc: 0.8962  Time: 33.5817
val Loss: 0.2927 Acc: 0.8949  Time: 60.1191

Epoch 2/10
----------
Iter 0/201 (Epoch 1), Train Loss = 0.166
Iter 50/201 (Epoch 1), Train Loss = 0.179
Iter 100/201 (Epoch 1), Train Loss = 0.289
Iter 150/201 (Epoch 1), Train Loss = 0.062
Iter 200/201 (Epoch 1), Train Loss = 0.007
train Loss: 0.1378 Acc: 0.9444  Time: 93.2938
val Loss: 0.2524 Acc: 0.9211  Time: 118.8886

Epoch 3/10
----------
Iter 0/201 (Epoch 2), Train Loss = 0.018
Iter 50/201 (Epoch 2), Train Loss = 0.041
Iter 100/201 (Epoch 2), Train Loss = 0.048
Iter 150/201 (Epoch 2), Train Loss = 0.042
Iter 200/201 (Epoch 2), Train Loss = 0.362
train Loss: 0.0800 Acc: 0.9696  Time: 152.0246
val Loss: 0.2228 Acc: 0.9170  Time: 179.2644

Epoch 4/10
----------
Iter 0/201 (Epoch 3), Train Loss = 0.040
Iter 50/201 (Epoch 3), Train Loss = 0.008
Iter 100/201 (Epoch 3), Train Loss = 0.085
Iter 150/201 (Epoch 3), Train Loss = 0.020
Iter 200/201 (Epoch 3), Train Loss = 0.361
train Loss: 0.0621 Acc: 0.9778  Time: 213.4156
val Loss: 0.2259 Acc: 0.9272  Time: 240.4777

Epoch 5/10
----------
Iter 0/201 (Epoch 4), Train Loss = 0.002
Iter 50/201 (Epoch 4), Train Loss = 0.038
Iter 100/201 (Epoch 4), Train Loss = 0.029
Iter 150/201 (Epoch 4), Train Loss = 0.202
Iter 200/201 (Epoch 4), Train Loss = 0.024
train Loss: 0.0520 Acc: 0.9808  Time: 275.2211
val Loss: 0.2791 Acc: 0.9037  Time: 301.8901

Epoch 6/10
----------
Iter 0/201 (Epoch 5), Train Loss = 0.016
Iter 50/201 (Epoch 5), Train Loss = 0.004
Iter 100/201 (Epoch 5), Train Loss = 0.014
Iter 150/201 (Epoch 5), Train Loss = 0.000
Iter 200/201 (Epoch 5), Train Loss = 0.380
train Loss: 0.0383 Acc: 0.9863  Time: 335.1696
val Loss: 0.3472 Acc: 0.8974  Time: 360.9584

Epoch 7/10
----------
Iter 0/201 (Epoch 6), Train Loss = 0.151
Iter 50/201 (Epoch 6), Train Loss = 0.165
Iter 100/201 (Epoch 6), Train Loss = 0.034
Iter 150/201 (Epoch 6), Train Loss = 0.002
Iter 200/201 (Epoch 6), Train Loss = 0.101
train Loss: 0.0550 Acc: 0.9800  Time: 394.4460
val Loss: 0.3211 Acc: 0.9170  Time: 420.8021

Epoch 8/10
----------
Iter 0/201 (Epoch 7), Train Loss = 0.008
Iter 50/201 (Epoch 7), Train Loss = 0.019
Iter 100/201 (Epoch 7), Train Loss = 0.021
Iter 150/201 (Epoch 7), Train Loss = 0.000
Iter 200/201 (Epoch 7), Train Loss = 0.006
train Loss: 0.0300 Acc: 0.9894  Time: 454.0427
val Loss: 0.2401 Acc: 0.9337  Time: 479.6816

Epoch 9/10
----------
Iter 0/201 (Epoch 8), Train Loss = 0.002
Iter 50/201 (Epoch 8), Train Loss = 0.002
Iter 100/201 (Epoch 8), Train Loss = 0.083
Iter 150/201 (Epoch 8), Train Loss = 0.045
Iter 200/201 (Epoch 8), Train Loss = 0.005
train Loss: 0.0219 Acc: 0.9920  Time: 513.1016
Epoch     9: reducing learning rate of group 0 to 3.0000e-05.
val Loss: 0.3908 Acc: 0.9068  Time: 539.9203

Epoch 10/10
----------
Iter 0/201 (Epoch 9), Train Loss = 0.011
Iter 50/201 (Epoch 9), Train Loss = 0.009
Iter 100/201 (Epoch 9), Train Loss = 0.002
Iter 150/201 (Epoch 9), Train Loss = 0.005
Iter 200/201 (Epoch 9), Train Loss = 0.000
train Loss: 0.0187 Acc: 0.9939  Time: 572.9529
val Loss: 0.2496 Acc: 0.9355  Time: 598.7445
Best model at 2 with lowest val loss 0.2228352453755559
Training complete in 9m 59s
Best val acc: 0.935464
